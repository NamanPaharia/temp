{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19765721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import queue\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BlenderbotTokenizer, BertTokenizerFast, RobertaTokenizerFast, GPT2TokenizerFast\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "# from rezero.transformer import RZTXEncoderLayer\n",
    "\n",
    "from models import SMI, is_ddp_module, WrappedSMI\n",
    "from utils import GEN_UNIQ_RUN_ID, pprint_args\n",
    "from datautils import DialogData, RMaxData\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad81ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.add_tokens('__eou__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367368a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "data_path = 'data/dailydialog'\n",
    "train_data = DialogData(data_path=data_path + '/dialogues_train.txt', tokenizer=tokenizer)\n",
    "valid_data = DialogData(data_path=data_path + '/dialogues_valid.txt', tokenizer=tokenizer)\n",
    "test_data = DialogData(data_path=data_path + '/dialogues_test.txt', tokenizer=tokenizer)\n",
    "\n",
    "# train_data = add_sep(train_data)\n",
    "# valid_data = add_sep(valid_data)\n",
    "# test_data  = add_sep(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "377d76d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import torch.nn as nn\n",
    "MAX_CTX_LEN = 300\n",
    "MAX_RESP_LEN = 301\n",
    "def proc_data(data):\n",
    "    mod_data = []\n",
    "    for i, j in enumerate(data):\n",
    "        ctx = torch.cat((j[0][-MAX_CTX_LEN:], torch.tensor([102])))\n",
    "        resp = torch.cat((j[1][1:][-MAX_RESP_LEN:], torch.tensor([102])))\n",
    "        label = j[1][-MAX_RESP_LEN:]\n",
    "        ctx_att = torch.ones(len(ctx))\n",
    "        resp_att = torch.ones(len(resp))\n",
    "        ctx_ids = nn.functional.pad(ctx, (0,MAX_CTX_LEN-len(ctx)+1))\n",
    "        resp_ids = nn.functional.pad(resp, (0,MAX_RESP_LEN-len(resp)))\n",
    "        label = nn.functional.pad(label, (0,MAX_RESP_LEN-len(resp)))\n",
    "        ctx_att_ids = nn.functional.pad(ctx_att, (0,MAX_CTX_LEN-len(ctx)))\n",
    "        resp_att_ids = nn.functional.pad(resp_att, (0,MAX_RESP_LEN-len(resp)))\n",
    "        mod_data.append({'ctx_ids': ctx_ids,\n",
    "                       'ctx_att': ctx_att_ids,\n",
    "                       'resp_ids': resp_ids,\n",
    "                       'resp_att': resp_att_ids,\n",
    "                        'label':label})\n",
    "    return mod_data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ca75508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(proc_data(train_data), batch_size=8, shuffle=False, num_workers=0)\n",
    "valid_dataloader = DataLoader(proc_data(valid_data), batch_size=8, shuffle=False, num_workers=0)\n",
    "test_dataloader = DataLoader(proc_data(test_data), batch_size=8, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96289c3",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52098123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mdevi                   \u001b[m  Wed Feb  9 02:17:42 2022  \u001b[1m\u001b[30m470.63.01\u001b[m\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mTesla P100-PCIE-12GB\u001b[m |\u001b[1m\u001b[31m 61'C\u001b[m, \u001b[1m\u001b[32m 96 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 5939\u001b[m / \u001b[33m12198\u001b[m MB | \u001b[1m\u001b[30mmithundas\u001b[m(\u001b[33m1671M\u001b[m) \u001b[1m\u001b[30mprasanta-am\u001b[m(\u001b[33m1745M\u001b[m) \u001b[1m\u001b[30mprasanta-am\u001b[m(\u001b[33m2517M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mTesla P100-PCIE-16GB\u001b[m |\u001b[31m 49'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    6\u001b[m / \u001b[33m16280\u001b[m MB | \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e8d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Transformer, Softmax\n",
    "from torch.optim import AdamW\n",
    "class EnDModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(tokenizer)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.d_model = 512\n",
    "        self.in_model = Transformer(self.d_model,num_encoder_layers=4, num_decoder_layers=4)\n",
    "        self.output_linear = nn.Linear(self.d_model, self.vocab_size)\n",
    "            \n",
    "    def forward(self, src_input, trg_input, e_mask=None, d_mask=None):\n",
    "        self.in_output = self.in_model(src_input, trg_input)\n",
    "        output = self.softmax(self.output_linear(self.in_output))\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54c392c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(tokenizer), 512)\n",
    "model = EnDModel()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "# Define Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e9b722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9906f1106fc44f48123391f53e64673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2478e8ee2d1c4b23a2fdc6ca971659c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 10.174951376794102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693c3fa0ffc94c0387f6d4011918abc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/884 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Loss 10.174220947118906\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f0a92d83e44dc3b19f63eabab77f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 10.174237710814928\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3df665ec94644c29fde3c717d6c38e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/884 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 10.174237710814928\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db4cf4c08c943eb94a2d0297d2507a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/884 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Loss 10.174220103483934\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edc79501f3e40c0a5f7140dfd2b62f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 10.174237710814928\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f16538a287e4bcda47583c59a5f5407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/884 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_loss = 'baseline'\n",
    "train_loss_epoch = []\n",
    "dev_loss_epoch = []\n",
    "max_epochs = 10\n",
    "write_step = 1\n",
    "best_metric = 0\n",
    "vocab_len = len(tokenizer)\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "best_train_loss = 0\n",
    "best_dev_loss = 0\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    train_loss = 0\n",
    "    dev_loss = 0\n",
    "    train_loss_set = []\n",
    "    dev_loss_set = []\n",
    "    train_bleu1 = 0\n",
    "    train_bleu2 = 0\n",
    "    train_bleu3 = 0\n",
    "    dev_bleu1 = 0\n",
    "    dev_bleu2 = 0\n",
    "    dev_bleu3 = 0\n",
    "    train_gold_resp = []\n",
    "    train_gen_resp = []\n",
    "    dev_gold_resp = []\n",
    "    dev_gen_resp = []\n",
    "    \n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(train_dataloader),total=len(train_dataloader)):\n",
    "        ctx_ids = embedding(batch['ctx_ids']).to(device)\n",
    "        resp_ids = embedding(batch['resp_ids']).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(ctx_ids,resp_ids)\n",
    "        \n",
    "        output_ids = torch.argmax(output,dim=2)\n",
    "        resp_shape = batch['resp_ids'].shape\n",
    "        loss = criterion(output.view(-1, vocab_len), batch['label'].view(resp_shape[0]*resp_shape[1]).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        train_gold_resp.extend(tokenizer.batch_decode(batch['resp_ids'].cpu().tolist(), skip_special_tokens=True))\n",
    "        train_gen_resp.extend(tokenizer.batch_decode(output_ids.cpu().tolist(), skip_special_token = True))\n",
    "        \n",
    "        \n",
    "    train_losses.append(train_loss/len(train_dataloader))\n",
    "    print(\"Train Loss\", train_loss/len(train_dataloader))\n",
    "        \n",
    "        \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(valid_dataloader),total=len(valid_dataloader)):\n",
    "            ctx_ids = embedding(batch['ctx_ids']).to(device)\n",
    "            resp_ids = embedding(batch['resp_ids']).to(device)\n",
    "\n",
    "            output = model(ctx_ids,resp_ids)\n",
    "        \n",
    "            output_ids = torch.argmax(output,dim=2)\n",
    "            resp_shape = batch['resp_ids'].shape\n",
    "            loss = criterion(output.view(-1, vocab_len), batch['label'].view(resp_shape[0]*resp_shape[1]).to(device))\n",
    "            dev_loss += loss.item()\n",
    "        \n",
    "            dev_gold_resp.extend(tokenizer.batch_decode(batch['resp_ids'].cpu().tolist(), skip_special_tokens=True))\n",
    "            dev_gen_resp.extend(tokenizer.batch_decode(output_ids.cpu().tolist(), skip_special_token = True))\n",
    "            \n",
    "        dev_losses.append(dev_loss/len(valid_dataloader))\n",
    "        print(\"Dev Loss\", dev_loss/len(valid_dataloader))\n",
    "        path = os.getcwd() + '/baseline_s2s.pth'\n",
    "        torch.save(model, path)\n",
    "        if best_dev_loss < dev_loss/len(valid_dataloader):\n",
    "            torch.save(model, os.getcwd() + '/baseline_s2s_best_loss.pth')\n",
    "            best_dev_loss = dev_loss/len(valid_dataloader)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aab711a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go right ahead.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_gold_resp[70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23b758be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]............................................................................................................................................................................................................................................................................................................'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_gen_resp[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78dd3ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.174951376794102,\n",
       " 10.17423791214283,\n",
       " 10.174237710814928,\n",
       " 10.174237710814928,\n",
       " 10.174237710814928,\n",
       " 10.174237710814928,\n",
       " 10.174237710814928,\n",
       " 10.174237710814928,\n",
       " 10.174237710814928,\n",
       " 10.174237710814928]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82686d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.174220947118906,\n",
       " 10.174220103483934,\n",
       " 10.174220103483934,\n",
       " 10.174220103483934,\n",
       " 10.174220103483934,\n",
       " 10.174220103483934,\n",
       " 10.174220103483934,\n",
       " 10.174220103483934,\n",
       " 10.174220103483934,\n",
       " 10.174220103483934]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e54180eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctx_ids': tensor([[ 101, 6160, 1010,  ...,    0,    0,    0],\n",
       "         [ 101, 6160, 1010,  ...,    0,    0,    0],\n",
       "         [ 101, 6160, 1010,  ...,    0,    0,    0],\n",
       "         [ 101, 6160, 1010,  ...,    0,    0,    0],\n",
       "         [ 101, 6160, 1010,  ...,    0,    0,    0]]),\n",
       " 'ctx_att': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.]]),\n",
       " 'resp_ids': tensor([[2821, 1010, 2009,  ...,    0,    0,    0],\n",
       "         [2053, 1010, 2065,  ...,    0,    0,    0],\n",
       "         [2129, 2172, 2003,  ...,    0,    0,    0],\n",
       "         [2048, 4595, 1012,  ...,    0,    0,    0],\n",
       "         [2821, 1010, 2009,  ...,    0,    0,    0]]),\n",
       " 'resp_att': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.]]),\n",
       " 'label': tensor([[ 101, 2821, 1010,  ...,    0,    0,    0],\n",
       "         [ 101, 2053, 1010,  ...,    0,    0,    0],\n",
       "         [ 101, 2129, 2172,  ...,    0,    0,    0],\n",
       "         [ 101, 2048, 4595,  ...,    0,    0,    0],\n",
       "         [ 101, 2821, 1010,  ...,    0,    0,    0]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb43a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(gold,gen):\n",
    "    bleu1, bleu2, bleu3 = 0,0,0\n",
    "    for i, j in tqdm(enumerate(gold), total = len(gold)):\n",
    "        bleu1 += sentence_bleu(j.split(\" \"), gen[i].split(\" \"), weights=(1, 0, 0, 0))\n",
    "        bleu2 += sentence_bleu(j.split(\" \"), gen[i].split(\" \"), weights=(0.5, 0.5, 0, 0))\n",
    "        bleu3 += sentence_bleu(j.split(\" \"), gen[i].split(\" \"), weights=(0.34, 0.33, 0.33, 0))\n",
    "    return bleu1/len(gold), bleu2/len(gold), bleu3/len(gold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
